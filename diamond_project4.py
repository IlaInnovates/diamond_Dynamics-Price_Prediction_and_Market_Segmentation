# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O_LvW5FLHP3N5UpZIEbxWfhpLe4ydJoT

# STEP 1: DATA LOADING & BASIC UNDERSTANDING

## STEP 1.1: Import Libraries
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import warnings
warnings.filterwarnings('ignore')

import sklearn
print(sklearn.__version__)

"""## STEP 1.2: Load the Dataset"""

df = pd.read_csv("diamonds.csv")

"""## STEP 1.3: Basic Inspection"""

df.head()

df.shape

df.info()

df.describe()

"""## STEP 1.4: Check Missing & Invalid Values"""

df.isnull().sum()

(df[['x','y','z']] == 0).sum()

"""# STEP 2: DATA PREPROCESSING

## STEP 2.1: Handle Invalid Zero Values in x, y, z
"""

# Replace zero values with NaN
df[['x', 'y', 'z']] = df[['x', 'y', 'z']].replace(0, np.nan)

"""## STEP 2.2: Check Missing Values Again"""

df.isnull().sum()

"""## STEP 2.3: Impute Missing Values (Median Strategy)"""

# Impute missing values with median
for col in ['x', 'y', 'z']:
    df[col].fillna(df[col].median(), inplace=True)

"""## STEP 2.4: Final Validation Check"""

df.isnull().sum()

(df[['x','y','z']] == 0).sum()

"""# STEP 3: OUTLIER HANDLING

## STEP 3.1: Select Numerical Columns
"""

num_cols = ['carat', 'price', 'x', 'y', 'z']

"""## STEP 3.2: Visual Outlier Detection (Boxplots)"""

plt.figure(figsize=(15, 8))

for i, col in enumerate(num_cols, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

## STEP 3.3: Outlier Detection Using IQR Method

def remove_outliers_iqr(df, columns):
    df_clean = df.copy()

    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1

        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR

        df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]

    return df_clean

"""## STEP 3.4: Apply Outlier Removal"""

df_outlier_removed = remove_outliers_iqr(df, num_cols)

"""## STEP 3.5: Compare Dataset Size (IMPORTANT)"""

print("Before outlier removal:", df.shape)
print("After outlier removal:", df_outlier_removed.shape)

"""## STEP 3.6: Use Clean Dataset Going Forward"""

df = df_outlier_removed.copy()

"""# STEP 4: SKEWNESS HANDLING

## STEP 4.1: Check Skewness of Numerical Features
"""

df[num_cols].skew()

"""## STEP 4.2: Visualize Distribution Before Transformation"""

plt.figure(figsize=(15, 8))

for i, col in enumerate(num_cols, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()

"""## STEP 4.3: Apply Log Transformation"""

df['price_log'] = np.log1p(df['price'])
df['carat_log'] = np.log1p(df['carat'])
df['x_log'] = np.log1p(df['x'])
df['y_log'] = np.log1p(df['y'])
df['z_log'] = np.log1p(df['z'])

"""## STEP 4.4: Check Skewness After Transformation"""

df[['price_log', 'carat_log', 'x_log', 'y_log', 'z_log']].skew()

"""## STEP 4.5: Visualize After Transformation"""

plt.figure(figsize=(15, 8))

log_cols = ['price_log', 'carat_log', 'x_log', 'y_log', 'z_log']

for i, col in enumerate(log_cols, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()

"""# STEP 5: EXPLORATORY DATA ANALYSIS (EDA)

## STEP 5.1: Distribution Plots (Numerical Features)
"""

num_features = ['price', 'carat', 'x', 'y', 'z']

plt.figure(figsize=(15, 8))

for i, col in enumerate(num_features, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()

"""## STEP 5.2: Count Plots (Categorical Features)"""

cat_features = ['cut', 'color', 'clarity']

plt.figure(figsize=(15, 5))

for i, col in enumerate(cat_features, 1):
    plt.subplot(1, 3, i)
    sns.countplot(x=df[col])
    plt.title(f'Count Plot of {col}')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""## STEP 5.3: Price vs Categorical Features (Boxplots)"""

plt.figure(figsize=(18, 5))

for i, col in enumerate(cat_features, 1):
    plt.subplot(1, 3, i)
    sns.boxplot(x=df[col], y=df['price'])
    plt.title(f'Price vs {col}')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""## STEP 5.4: Correlation Heatmap (Numerical Features)"""

plt.figure(figsize=(10, 6))
sns.heatmap(df[num_features].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""## STEP 5.5: Scatterplot Matrix (Pairplot)"""

sns.pairplot(df[['carat', 'x', 'y', 'z', 'price']])
plt.show()

"""## STEP 5.6: Carat vs Price (Regression Plot)"""

sns.regplot(x='carat', y='price', data=df, scatter_kws={'alpha':0.3})
plt.title('Carat vs Price')
plt.show()

"""## STEP 5.7: Average Price by Categories"""

plt.figure(figsize=(18, 5))

for i, col in enumerate(cat_features, 1):
    plt.subplot(1, 3, i)
    avg_price = df.groupby(col)['price'].mean()
    avg_price.plot(kind='bar')
    plt.title(f'Average Price by {col}')
    plt.ylabel('Average Price')

plt.tight_layout()
plt.show()

"""# STEP 6: FEATURE ENGINEERING

## STEP 6.1: Convert Price from USD to INR
"""

USD_TO_INR = 83

df['price_inr'] = df['price'] * USD_TO_INR

"""## STEP 6.2: Create Volume Feature"""

df['volume'] = df['x'] * df['y'] * df['z']

"""## STEP 6.3: Price per Carat"""

df['price_per_carat'] = df['price'] / df['carat']

"""## STEP 6.4: Dimension Ratio"""

df['dimension_ratio'] = (df['x'] + df['y']) / (2 * df['z'])

"""## STEP 6.5: Carat Category (Categorical Feature)"""

def carat_category(carat):
    if carat < 0.5:
        return 'Light'
    elif carat <= 1.5:
        return 'Medium'
    else:
        return 'Heavy'

df['carat_category'] = df['carat'].apply(carat_category)

"""## STEP 6.6: Quick Verification"""

df.head()

df[['volume', 'price_per_carat', 'dimension_ratio']].describe()

"""# STEP 7: FEATURE SELECTION & ENCODING

## PART A: FEATURE SELECTION (Correlation-Based)

## STEP 7.1: Correlation with Target (price)
"""

corr_with_price = df.corr(numeric_only=True)['price'].sort_values(ascending=False)
corr_with_price

"""## STEP 7.2: Select Final Numerical Features"""

num_features_selected = [
    'carat',
    'volume',
    'price_per_carat',
    'dimension_ratio',
    'x',
    'y',
    'z',
    'depth',
    'table'
]

"""## PART B: DEFINE TARGET VARIABLE"""

target = 'price_inr'

"""## PART C: CATEGORICAL & NUMERICAL FEATURES

## STEP 7.3: Define Feature Groups
"""

categorical_features = ['cut', 'color', 'clarity', 'carat_category']
numerical_features = num_features_selected

"""## PART D: ENCODING STRATEGY

## STEP 7.4: Define Ordinal Categories
"""

cut_order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']
color_order = ['J', 'I', 'H', 'G', 'F', 'E', 'D']
clarity_order = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']

"""## STEP 7.5: Create Preprocessing Pipeline"""

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat_ord', OrdinalEncoder(categories=[cut_order, color_order, clarity_order]),
         ['cut', 'color', 'clarity']),
        ('cat_ohe', OneHotEncoder(drop='first', sparse_output=False),
         ['carat_category'])
    ]
)

"""## STEP 7.6: Prepare Final X and y"""

X = df[categorical_features + numerical_features]
y = df[target]

"""## STEP 7.7: Train-Test Split"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

"""## STEP 7.8: Apply Preprocessing"""

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

"""## FINAL VERIFICATION"""

X_train_processed.shape, X_test_processed.shape

"""# STEP 8: REGRESSION MODEL BUILDING & EVALUATION

## STEP 8.1: Import Models
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor

from xgboost import XGBRegressor

"""## STEP 8.2: Evaluation Function (Reusable)"""

def evaluate_model(name, model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"\n{name}")
    print("-" * 30)
    print(f"MAE  : {mae:.2f}")
    print(f"MSE  : {mse:.2f}")
    print(f"RMSE : {rmse:.2f}")
    print(f"R²   : {r2:.4f}")

    return [name, mae, mse, rmse, r2]

"""## STEP 8.3: Initialize Models"""

models = [
    ("Linear Regression", LinearRegression()),
    ("Decision Tree", DecisionTreeRegressor(random_state=42)),
    ("Random Forest", RandomForestRegressor(n_estimators=100, random_state=42)),
    ("KNN", KNeighborsRegressor(n_neighbors=5)),
    ("XGBoost", XGBRegressor(
        n_estimators=200,
        learning_rate=0.1,
        max_depth=6,
        objective='reg:squarederror',
        random_state=42
    ))
]

"""## STEP 8.4: Train & Evaluate All Models"""

results = []

for name, model in models:
    results.append(
        evaluate_model(
            name,
            model,
            X_train_processed,
            X_test_processed,
            y_train,
            y_test
        )
    )

"""## STEP 8.5: Compare Model Performance"""

results_df = pd.DataFrame(
    results,
    columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2']
)

results_df.sort_values(by='R2', ascending=False)

"""## STEP 8.6: Save the Best Model (Pickle)"""

import joblib

best_model = models[-1][1]  # XGBoost

joblib.dump(best_model, "best_price_prediction_model.pkl")



"""## STEP 8.7: Save Preprocessor"""

joblib.dump(preprocessor, "preprocessor.pkl")

import joblib

joblib.dump(preprocessor, "preprocessor.pkl")
joblib.dump(model, "model.pkl")

"""# STEP 9: ANN REGRESSION MODEL (TensorFlow)

## STEP 9.1: Import TensorFlow Libraries
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

"""## STEP 9.2: Build the ANN Model"""

ann_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_processed.shape[1],)),
    Dropout(0.2),

    Dense(64, activation='relu'),
    Dropout(0.2),

    Dense(32, activation='relu'),

    Dense(1)  # Output layer
])

"""## STEP 9.3: Compile the Model"""

ann_model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

"""## STEP 9.4: Early Stopping (Prevent Overfitting)"""

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

"""## STEP 9.5: Train the ANN Model"""

history = ann_model.fit(
    X_train_processed,
    y_train,
    validation_split=0.2,
    epochs=40,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

"""## STEP 9.6: Evaluate ANN Model"""

y_pred_ann = ann_model.predict(X_test_processed)

mae_ann = mean_absolute_error(y_test, y_pred_ann)
mse_ann = mean_squared_error(y_test, y_pred_ann)
rmse_ann = np.sqrt(mse_ann)
r2_ann = r2_score(y_test, y_pred_ann)

print("ANN Regression Model Performance")
print("-" * 35)
print(f"MAE  : {mae_ann:.2f}")
print(f"MSE  : {mse_ann:.2f}")
print(f"RMSE : {rmse_ann:.2f}")
print(f"R²   : {r2_ann:.4f}")

"""## STEP 9.7: Save ANN Model"""

ann_model.save("ann_price_prediction_model.h5")

"""# STEP 10: CLUSTERING – MARKET SEGMENTATION

## STEP 10.1: Select Features for Clustering
"""

cluster_features = [
    'carat',
    'volume',
    'price_per_carat',
    'dimension_ratio',
    'cut',
    'color',
    'clarity'
]

cluster_features = df[
    ["carat", "depth", "table", "x", "y", "z", "volume"]
]

cluster_features

# Redefine cluster_features as a list of strings containing all relevant columns
# This includes numerical features previously identified and categorical features needed for encoding
cluster_features_list = [
    'carat', 'depth', 'table', 'x', 'y', 'z', 'volume',
    'price_per_carat', 'dimension_ratio',
    'cut', 'color', 'clarity'
]
df_cluster = df[cluster_features_list].copy()

"""## STEP 10.2: Encode Categorical Variables (For Clustering)"""

ordinal_encoder = OrdinalEncoder(
    categories=[cut_order, color_order, clarity_order]
)

df_cluster[['cut', 'color', 'clarity']] = ordinal_encoder.fit_transform(
    df_cluster[['cut', 'color', 'clarity']]
)

"""## STEP 10.3: Feature Scaling"""

scaler = StandardScaler()
df_cluster_scaled = scaler.fit_transform(df_cluster)

"""## STEP 10.4: Find Optimal Clusters – Elbow Method"""

from sklearn.cluster import KMeans

inertia = []

K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_cluster_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(6, 4))
plt.plot(K_range, inertia, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

"""## STEP 10.5: Train Final K-Means Model"""

kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(df_cluster_scaled)

"""## STEP 10.6: Cluster Analysis"""

cluster_summary = df.groupby('cluster')[['carat', 'price_inr']].mean()
cluster_summary

"""## STEP 10.7: Cluster Naming (SCORING PART)"""

cluster_names = {
    0: "Affordable Small Diamonds",
    1: "Mid-range Balanced Diamonds",
    2: "Premium Heavy Diamonds"
}

df['cluster_name'] = df['cluster'].map(cluster_names)

"""## STEP 10.8: PCA for Cluster Visualization"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_data = pca.fit_transform(df_cluster_scaled)

plt.figure(figsize=(6, 5))
sns.scatterplot(
    x=pca_data[:, 0],
    y=pca_data[:, 1],
    hue=df['cluster_name'],
    palette='Set2'
)
plt.title('Diamond Clusters (PCA Projection)')
plt.show()

"""## STEP 10.9: Save Clustering Models"""

joblib.dump(kmeans, "diamond_clustering_model.pkl")
joblib.dump(scaler, "cluster_scaler.pkl")
joblib.dump(ordinal_encoder, "cluster_encoder.pkl")

import joblib
from sklearn.cluster import KMeans

# Use the SAME processed features used for clustering
X_cluster = df_cluster_scaled # Use the correctly processed and scaled features

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X_cluster)

joblib.dump(kmeans, "cluster_model.pkl")

print("cluster_model.pkl saved successfully")